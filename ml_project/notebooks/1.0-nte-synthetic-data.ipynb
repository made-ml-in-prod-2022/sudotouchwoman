{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from plotly.offline import iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebooks contains simple synthetic data generation pipeline:\n",
    "First, the data is read from the default location and stored in a `DataFrame`. As all the features are numeric\n",
    "and distributed quite normally, the basic statistics are collected and used for generation\n",
    "(new data points are sampled from normal distribution). In the last section, synthetic data points are compared with\n",
    "the original ones using PCA\n",
    "\n",
    "__The outputs of this notebook were cleared in order to reduce the size__\n",
    "Feel free to run it yourself, but do not forget to download the data first (e.g., run the training pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = \"../data/raw/breast-cancer-dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "FEATURE_TYPES = (\n",
    "    \"radius\",\n",
    "    \"texture\",\n",
    "    \"peri\",\n",
    "    \"area\",\n",
    "    \"smoothness\",\n",
    "    \"compactness\",\n",
    "    \"concavity\",\n",
    "    \"concave_points\",\n",
    "    \"symmetry\",\n",
    "    \"fractal_dim\",\n",
    ")\n",
    "\n",
    "FEATURES = chain(\n",
    "    map(lambda x: f\"{x}_mean\", FEATURE_TYPES),\n",
    "    map(lambda x: f\"{x}_se\", FEATURE_TYPES),\n",
    "    map(lambda x: f\"{x}_worst\", FEATURE_TYPES),\n",
    ")\n",
    "\n",
    "FEATURES = tuple(FEATURES)\n",
    "DEFAULT_COLUMN_NAMES = [\"id\", \"diag\", *FEATURES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(raw_data_path, names=DEFAULT_COLUMN_NAMES).drop(columns=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, features = raw_data.diag, raw_data.drop(columns=[\"diag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Stats:\n",
    "    mean: np.ndarray\n",
    "    std: np.ndarray\n",
    "\n",
    "    def normalize(self, data: np.ndarray) -> np.ndarray:\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "\n",
    "def extract_statistics(df: pd.DataFrame) -> Stats:\n",
    "    return Stats(mean=df.to_numpy().mean(axis=0), std=df.to_numpy().std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stats = extract_statistics(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericFaker:\n",
    "    def __init__(self, stats: Stats, seed: int = 1) -> None:\n",
    "        self.stats = stats\n",
    "        self.rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    def generate_points(self, n_points: int) -> np.ndarray:\n",
    "        fake_features = tuple(\n",
    "            self.rng.normal(mean, std, size=n_points).T for mean, std\n",
    "            in zip(self.stats.mean, self.stats.std)\n",
    "        )\n",
    "\n",
    "        return np.column_stack(fake_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = NumericFaker(stats=features_stats, seed=42)\n",
    "fakes = gen.generate_points(n_points=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = pd.DataFrame(fakes, columns=FEATURES)\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(trues: np.ndarray, fakes: np.ndarray):\n",
    "    frames = (\n",
    "        (\n",
    "            go.Histogram(\n",
    "                x=real, name=\"Real\", hovertext=feature,\n",
    "                opacity=0.8, histnorm=\"probability\",\n",
    "                legendgroup=\"Real\", showlegend=False,\n",
    "                marker=dict(color=\"crimson\")\n",
    "            ),\n",
    "            go.Histogram(\n",
    "                x=fake, name=\"Fake\", hovertext=feature,\n",
    "                opacity=0.8, histnorm=\"probability\",\n",
    "                legendgroup=\"Fake\", showlegend=False,\n",
    "                marker=dict(color=\"mediumseagreen\")\n",
    "            )\n",
    "        )\n",
    "    for real, fake, feature in zip(trues, fakes, FEATURES)\n",
    "    )\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=10)\n",
    "    fig.update_layout(\n",
    "        width=1500, height=1000,\n",
    "        title_text=\"Original vs. Fake data comparison\"\n",
    "    )\n",
    "\n",
    "    for i, (true_hist, fakes_hist) in enumerate(frames):\n",
    "        row, col = i // 10 + 1, i % 10 + 1\n",
    "        fig.append_trace(true_hist, row, col)\n",
    "        fig.append_trace(fakes_hist, row, col)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = NumericFaker(stats=features_stats, seed=42)\n",
    "fig = compare_distributions(features.to_numpy().T, gen.generate_points(n_points=500).T)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above demonstrates that generated data is pretty simular to the original. This is possible because the original features have almost normal distribution (if these had, say, uniform or bimodal distribution, such sampling would have not yielded such pretty results).\n",
    "\n",
    "Importand preprocessing step before applying PCA is normalization (it prevents the transformed data from being skewed due to varying feature scales)\n",
    "This can be easily done as we already have the required statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = NumericFaker(stats=features_stats, seed=42)\n",
    "fakes = gen.generate_points(n_points=400)\n",
    "\n",
    "original_normed = features_stats.normalize(features.to_numpy())\n",
    "fake_normed = features_stats.normalize(fakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "original_3d = pca.fit_transform(features.to_numpy())\n",
    "fake_3d = pca.transform(fakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_graph(real, fake, title: str = \"Original vs. Fake after PCA transform\"):\n",
    "    dfs = (pd.DataFrame(\n",
    "        {\n",
    "            \"PC1\": data[:, 0],\n",
    "            \"PC2\": data[:, 1],\n",
    "            \"PC3\": data[:, 2],\n",
    "        }\n",
    "    ) for data in (real, fake))\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.update_layout(\n",
    "        width=1000, height=900,\n",
    "        title_text=title\n",
    "    )\n",
    "\n",
    "    for df, name in zip(dfs, (\"Real\", \"Fake\")):\n",
    "        fig.add_scatter3d(x=df.PC1, y=df.PC2, z=df.PC3, name=name, mode='markers', opacity=0.6)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def pca_pipeline(stats: Stats, original: np.ndarray, fake_points: int, seed: int = 1, scale: bool = False) -> go.Figure:\n",
    "    gen = NumericFaker(stats=stats, seed=seed)\n",
    "    fakes = gen.generate_points(fake_points)\n",
    "    pca = PCA(n_components=3)\n",
    "\n",
    "    if scale:\n",
    "        original, fakes = map(stats.normalize, (original, fakes))\n",
    "\n",
    "    original_3d = pca.fit_transform(original)\n",
    "    fake_3d = pca.transform(fakes)\n",
    "\n",
    "    return pca_graph(\n",
    "        original_3d, fake_3d,\n",
    "        title=f\"Original vs. Fake after PCA transform ({'normalized' if scale else 'no scaling'})\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(pca_pipeline(stats=features_stats, original=features.to_numpy(), fake_points=200, seed=42, scale=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iplot(pca_pipeline(stats=features_stats, original=features.to_numpy(), fake_points=200, seed=42, scale=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting results: when fitted on stardard-scaled data (this said, data with unit std and zero mean),\n",
    "fake data forms a 3-dimensional gaussoid (I guess?), so it looks like PCA transform does preserve the distribution properties of the\n",
    "fakes (this sample essentially forms a 30-dimensional gaussoid).\n",
    "However, when scaling is not performed, the data is skewed and fakes are distinguished easily.\n",
    "\n",
    "It was a nice Jupyter time and in the following cell dump of generated data is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_FILE = \"synthetic-features.csv\"\n",
    "\n",
    "pd.DataFrame(gen.generate_points(n_points=50), columns=FEATURES).to_csv(DUMP_FILE, sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d9ef825431e3983a65f9f87cf8edab17a5efccaab5f51baa1aa404d0314f6f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
