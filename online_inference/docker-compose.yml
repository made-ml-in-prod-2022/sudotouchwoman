version: '3'
services:
    server-gunicorn:
        container_name: online_inference_backend
        image: online_inference_backend
        build:
            context: .
            dockerfile: ./Dockerfile
        env_file: env/dev.env
        command: 'gunicorn  --workers=1 --threads=4 --timeout=300 --bind 0.0.0.0:5000 wsgi:app'
        ports:
            - 5000:5000
        networks:
            - online_inference_network

networks:
  online_inference_network:
    driver: bridge
